{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COS 429 Final Project\n",
    "## VGG Face - DFI Manipulations\n",
    "\n",
    "AWS setup:\n",
    "- `source activate theano_p36`\n",
    "- `conda install -c anaconda pillow`\n",
    "- `conda install h5py`\n",
    "- `conda install scikit-learn`\n",
    "- `jupyter notebook`\n",
    "- `scp -i cos429.pem *.py ubuntu@...:~/cos429/`\n",
    "\n",
    "Download the VGG-FACE pre-trained weights for Keras here: https://drive.google.com/file/d/0B4ChsjFJvew3NkF0dTc1OGxsOFU/view.\n",
    "\n",
    "Before stopping the instance, remember to download the latest .ipynb file for the GitHub. Terminate the instance to delete all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run once at start of program\n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys  \n",
    "# os.environ['THEANO_FLAGS'] = \"device=gpu1\"    \n",
    "# import theano\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = 'vgg/vgg-face-keras.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This network architecture is derived from Table 3 of the CNN described in Parkhi et al. \n",
    "# and based on Keras code provided in https://gist.github.com/EncodeTS/6bbe8cb8bebad7a672f0d872561782d9\n",
    "\n",
    "def vgg_face(weights_path=None):\n",
    "    img = Input(shape=(3, 224, 224))\n",
    "\n",
    "    pad1_1 = ZeroPadding2D(padding=(1, 1))(img)\n",
    "    conv1_1 = Convolution2D(64, (3, 3), activation='relu', name='conv1_1')(pad1_1)\n",
    "    pad1_2 = ZeroPadding2D(padding=(1, 1))(conv1_1)\n",
    "    conv1_2 = Convolution2D(64, (3, 3), activation='relu', name='conv1_2')(pad1_2)\n",
    "    pool1 = MaxPooling2D((2, 2), strides=(2, 2))(conv1_2)\n",
    "\n",
    "    pad2_1 = ZeroPadding2D((1, 1))(pool1)\n",
    "    conv2_1 = Convolution2D(128, (3, 3), activation='relu', name='conv2_1')(pad2_1)\n",
    "    pad2_2 = ZeroPadding2D((1, 1))(conv2_1)\n",
    "    conv2_2 = Convolution2D(128, (3, 3), activation='relu', name='conv2_2')(pad2_2)\n",
    "    pool2 = MaxPooling2D((2, 2), strides=(2, 2))(conv2_2)\n",
    "\n",
    "    pad3_1 = ZeroPadding2D((1, 1))(pool2)\n",
    "    conv3_1 = Convolution2D(256, (3, 3), activation='relu', name='conv3_1')(pad3_1)\n",
    "    pad3_2 = ZeroPadding2D((1, 1))(conv3_1)\n",
    "    conv3_2 = Convolution2D(256, (3, 3), activation='relu', name='conv3_2')(pad3_2)\n",
    "    pad3_3 = ZeroPadding2D((1, 1))(conv3_2)\n",
    "    conv3_3 = Convolution2D(256, (3, 3), activation='relu', name='conv3_3')(pad3_3)\n",
    "    pool3 = MaxPooling2D((2, 2), strides=(2, 2))(conv3_3)\n",
    "\n",
    "    pad4_1 = ZeroPadding2D((1, 1))(pool3)\n",
    "    conv4_1 = Convolution2D(512, (3, 3), activation='relu', name='conv4_1')(pad4_1)\n",
    "    pad4_2 = ZeroPadding2D((1, 1))(conv4_1)\n",
    "    conv4_2 = Convolution2D(512, (3, 3), activation='relu', name='conv4_2')(pad4_2)\n",
    "    pad4_3 = ZeroPadding2D((1, 1))(conv4_2)\n",
    "    conv4_3 = Convolution2D(512, (3, 3), activation='relu', name='conv4_3')(pad4_3)\n",
    "    pool4 = MaxPooling2D((2, 2), strides=(2, 2))(conv4_3)\n",
    "\n",
    "    pad5_1 = ZeroPadding2D((1, 1))(pool4)\n",
    "    conv5_1 = Convolution2D(512, (3, 3), activation='relu', name='conv5_1')(pad5_1)\n",
    "    pad5_2 = ZeroPadding2D((1, 1))(conv5_1)\n",
    "    conv5_2 = Convolution2D(512, (3, 3), activation='relu', name='conv5_2')(pad5_2)\n",
    "    pad5_3 = ZeroPadding2D((1, 1))(conv5_2)\n",
    "    conv5_3 = Convolution2D(512, (3, 3), activation='relu', name='conv5_3')(pad5_3)\n",
    "    pool5 = MaxPooling2D((2, 2), strides=(2, 2))(conv5_3)\n",
    "\n",
    "    # These layers are used in the original VGG Face paper for their dataset of 2,622 individuals\n",
    "    # The output of the previous layer is the 4096-dimensional face descriptor\n",
    "    fc6 = Convolution2D(4096, (7, 7), activation='relu', name='fc6')(pool5)\n",
    "    fc6_drop = Dropout(0.5)(fc6)\n",
    "    fc7 = Convolution2D(4096, (1, 1), activation='relu', name='fc7')(fc6_drop)\n",
    "    fc7_drop = Dropout(0.5)(fc7)\n",
    "    fc8 = Convolution2D(2622, (1, 1), name='fc8')(fc7_drop)\n",
    "    flat = Flatten()(fc8)\n",
    "    out = Activation('softmax')(flat)\n",
    "\n",
    "    model = Model(inputs=img, outputs=out)\n",
    "\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Returns model that for the 4096-dimensional face descriptor \n",
    "def partial_vgg_face():\n",
    "    model = vgg_face(weights_path)\n",
    "    layer_name = 'fc7'\n",
    "    partial_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer(layer_name).output)\n",
    "    return partial_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "% load_ext autoreload\n",
    "% aimport experiment\n",
    "% aimport manipulations\n",
    "% autoreload 1\n",
    "\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy import ndimage\n",
    "from scipy.stats import mode\n",
    "import h5py\n",
    "\n",
    "import manipulations\n",
    "import experiment\n",
    "from manipulations import ManipulationInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainedModel:\n",
    "    def __init__(self, num_train):\n",
    "        self.num_train = num_train\n",
    "        \n",
    "        print('Loading model')\n",
    "        self.model = partial_vgg_face()\n",
    "        \n",
    "        print('Loading training dataset')\n",
    "        self.min_faces_per_person = 20\n",
    "        train_data, train_targets, _, _ = experiment.get_lfw_dataset(self.min_faces_per_person, num_train, color=True, size=224)\n",
    "        \n",
    "        # Train\n",
    "        print('Training')\n",
    "        time1 = time.clock()\n",
    "        self.num_faces = len(np.unique(train_targets))\n",
    "        self.num_examples_per_face = int(len(train_targets) / self.num_faces)\n",
    "        self.train_descriptors = self.get_descriptors(train_data)\n",
    "        time2 = time.clock()\n",
    "        self.train_time = time2 - time1\n",
    "        print('Training time: %f' % self.train_time)\n",
    "\n",
    "    # Prediction with nearest-k neighbors and cosine similarity\n",
    "    def predict(self, test_descriptors, k):\n",
    "        predictions = []\n",
    "\n",
    "        for d in test_descriptors:\n",
    "            # Use cosine similarity\n",
    "            distances = [cosine(self.train_descriptors[i], d) for i in range(len(self.train_descriptors))]\n",
    "\n",
    "            # The closest k vote instead of average\n",
    "            closest_k = np.asarray(distances).argsort()[:k] // self.num_examples_per_face\n",
    "            predictions.append(mode(closest_k).mode)\n",
    "        return np.asarray(predictions).flatten()\n",
    "\n",
    "    def predict_train(self, k):\n",
    "        return self.predict(self.train_descriptors, k)\n",
    "\n",
    "    def get_descriptors(self, data):\n",
    "        data = data.transpose((0, 3, 1, 2))\n",
    "        descriptors = self.model.predict(data, verbose=1)\n",
    "        return np.squeeze(descriptors)\n",
    "\n",
    "    # Alternative prediction method with average and L2 Euclidean distance\n",
    "    # def predict_with_mean(train_descriptors, test_descriptors, num_examples_per_face, threshold=None):\n",
    "    #     predictions = []\n",
    "    #     mean_train_descriptors = np.mean(np.reshape(train_descriptors, (-1, num_examples_per_face, 4096)), axis=1)\n",
    "\n",
    "    #     for d in test_descriptors:\n",
    "    #         distances = [np.linalg.norm(mean_train_descriptors[i] - d) for i in range(len(mean_train_descriptors))]\n",
    "    #         predictions.append(np.argmin(distances))\n",
    "    #     return np.asarray(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(trained_model, manipulation_info):\n",
    "    _, _, test_data, test_targets = experiment.get_lfw_dfi_dataset(trained_model.min_faces_per_person, trained_model.num_train, manipulation_info, color=True, size=224)\n",
    "    test_descriptors = trained_model.get_descriptors(test_data)\n",
    "\n",
    "    # Test\n",
    "    print('Testing')\n",
    "    time1 = time.clock()\n",
    "    train_predictions_1 = trained_model.predict_train(1)\n",
    "    train_predictions_3 = trained_model.predict_train(3)\n",
    "    train_predictions_5 = trained_model.predict_train(5)\n",
    "    train_predictions_7 = trained_model.predict_train(7)\n",
    "    # train_accuracy = experiment.compute_accuracy(train_predictions, test_targets)\n",
    "    train_accuracy_1 = experiment.compute_accuracy(train_predictions_1, train_targets)\n",
    "    train_accuracy_3 = experiment.compute_accuracy(train_predictions_3, train_targets)\n",
    "    train_accuracy_5 = experiment.compute_accuracy(train_predictions_5, train_targets)\n",
    "    train_accuracy_7 = experiment.compute_accuracy(train_predictions_7, train_targets)\n",
    "    \n",
    "    # Predict test_descriptors\n",
    "    test_predictions_1 = trained_model.predict_descriptors(test_descriptors, 1)\n",
    "    test_predictions_3 = trained_model.predict_descriptors(test_descriptors, 3)\n",
    "    test_predictions_5 = trained_model.predict_descriptors(test_descriptors, 5)\n",
    "    test_predictions_7 = trained_model.predict_descriptors(test_descriptors, 7)\n",
    "    # test_accuracy = experiment.compute_accuracy(test_predictions, test_targets)\n",
    "    test_accuracy_1 = experiment.compute_accuracy(test_predictions_1, test_targets)\n",
    "    test_accuracy_3 = experiment.compute_accuracy(test_predictions_3, test_targets)\n",
    "    test_accuracy_5 = experiment.compute_accuracy(test_predictions_5, test_targets)\n",
    "    test_accuracy_7 = experiment.compute_accuracy(test_predictions_7, test_targets)\n",
    "    time2 = time.clock()\n",
    "    test_time = time2 - time1\n",
    "    \n",
    "    train_accuracy = { \"k=1\": train_accuracy_1, \n",
    "                      \"k=3\": train_accuracy_3, \n",
    "                      \"k=5\": train_accuracy_5, \n",
    "                      \"k=7\": train_accuracy_7 }\n",
    "    \n",
    "    test_accuracy = { \"k=1\": test_accuracy_1, \n",
    "                      \"k=3\": test_accuracy_3, \n",
    "                      \"k=5\": test_accuracy_5, \n",
    "                      \"k=7\": test_accuracy_7 }\n",
    "    \n",
    "    # Print results.\n",
    "    num_faces = len(np.unique(train_targets))\n",
    "    model_name = 'VGG_FACE'\n",
    "    print(\"Manipulation info: %s\" % str(manipulation_info))\n",
    "    print(\"Recognition Algorithm: %s\" % model_name)\n",
    "    print(\"Number of distinct faces: %d\" % num_faces)\n",
    "    print(\"Chance rate: %f\" % (1 / num_faces))\n",
    "    print(\"Train accuracy: %s\" % train_accuracy)\n",
    "    print(\"Test accuracy: %s\" % test_accuracy)\n",
    "    print(\"Training Time: %s sec\" % train_time)\n",
    "    print(\"Testing Time: %s sec\" % test_time)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    \n",
    "    return {\n",
    "        \"Manipulation Type\": manipulation_info.type,\n",
    "        \"Manipulation Parameters\": manipulation_info.parameters,\n",
    "        \"Recognition Algorithm\": model_name,\n",
    "        \"Min Faces Per Person\": min_faces_per_person,\n",
    "        \"Number of Distinct Faces\": num_faces,\n",
    "        \"Chance Rate\": (1 / num_faces),\n",
    "        \"Train Accuracy\": train_accuracy,\n",
    "        \"Test Accuracy\": test_accuracy,\n",
    "        \"Training Time\": train_time,\n",
    "        \"Testing Time\": test_time,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num training examples: 3\n",
      "Loading model\n",
      "Loading training dataset\n",
      "Training\n",
      "186/186 [==============================] - 4s     \n",
      "Training time: 4.475616\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 4. the normalize function expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-c88848614988>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainedModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmanipulation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmanipulation_infos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanipulation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-d08158c6351c>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(trained_model, manipulation_info)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanipulation_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lfw_dfi_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_faces_per_person\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanipulation_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtest_descriptors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_descriptors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/429/code/experiment.py\u001b[0m in \u001b[0;36mget_lfw_dfi_dataset\u001b[0;34m(min_faces_per_person, num_train, manipulation_info, color, size)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mtest_data\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mmean_face\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[1;32m   1410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m     X = check_array(X, sparse_format, copy=copy,\n\u001b[0;32m-> 1412\u001b[0;31m                     estimator='the normalize function', dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m   1413\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n\u001b[0;32m--> 420\u001b[0;31m                              % (array.ndim, estimator_name))\n\u001b[0m\u001b[1;32m    421\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 4. the normalize function expected <= 2."
     ]
    }
   ],
   "source": [
    "manipulation_infos = [\n",
    "        ManipulationInfo(\"dfi\", {\"transform\": \"Senior\"}),\n",
    "        ManipulationInfo(\"dfi\", {\"transform\": \"Mustache\"}),\n",
    "    ]\n",
    "\n",
    "num_trains = [3, 10, 15, 19]\n",
    "for num_train in num_trains:\n",
    "    print(\"Num training examples: %d\" % num_train)\n",
    "    save_path = \"vgg_face_dfi_%d.csv\" % num_train\n",
    "    results = pd.DataFrame(columns=experiment.COLUMNS)\n",
    "    trained_model = TrainedModel(num_train)\n",
    "    for manipulation in manipulation_infos:\n",
    "        stats = run_experiment(trained_model, manipulation)\n",
    "        results.append(stats, index=False)\n",
    "        print(stats)\n",
    "        print()\n",
    "    results.to_csv(save_path, index=False)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Graphs\n",
    "\n",
    "# # Accuracy as a function of k for 3 training images\n",
    "# # With and without preprocessing\n",
    "# # Mean/Euclidean distance included as a line\n",
    "\n",
    "# accuracy = np.array([0.8808600634473035, 0.83785689108212902, 0.83080719069439546, 0.79978850898836795])\n",
    "# preprocessed_accuracy = np.array([0.96686640817765246, 0.96263658794501239, 0.94360239689813186, 0.9002467395135707])\n",
    "# k = np.array([1, 3, 5, 7])\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# width = 0.35\n",
    "# acc1 = ax.bar(k, accuracy, width, label='No preprocessing')\n",
    "# acc2 = ax.bar(k+width, preprocessed_accuracy, width, label='Preprocessed')\n",
    "# ax.plot([0, 8], [0.85442368699330273, 0.85442368699330273], 'k-', lw=1, linestyle='dashed')\n",
    "# ax.set_xticks(k)\n",
    "# ax.set_xlabel('$k$')\n",
    "# ax.set_ylabel('Accuracy')\n",
    "# ax.set_title('VGG_FACE accuracy on LFW with varying $k$ and 3 training images')\n",
    "# legend = ax.legend(handles=[acc1, acc2], loc=4)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
