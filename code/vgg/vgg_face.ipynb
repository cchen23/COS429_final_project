{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COS 429 Final Project\n",
    "## VGG Face\n",
    "\n",
    "Initial setup:\n",
    "- Create instance (p2.xlarge)\n",
    "- `scp` the .caffemodel and .prototxt files over\n",
    "- Create ssl cert and password for Jupyter notebook\n",
    "\n",
    "To get this up and running on AWS (after initial setup):\n",
    "- `sudo ssh -i thesis.pem -L 443:127.0.0.1:8888 ubuntu@...`\n",
    "- `127.0.0.1`\n",
    "- Password: cos429_russakovsky\n",
    "- `source activate theano_p36`\n",
    "- `conda install -c anaconda pillow`\n",
    "- `conda install h5py`\n",
    "- `conda install scikit-learn`\n",
    "- `jupyter notebook`\n",
    "- `scp -i cos429.pem *.py ubuntu@...:~/cos429/`\n",
    "\n",
    "This uses the Keras weights (hard to get caffemodel and t7 files working for caffe2/pytorch) for VGG_FACE, which was converted from vgg-face matconvnet model using as shown here: https://gist.github.com/EncodeTS/6bbe8cb8bebad7a672f0d872561782d9.\n",
    "\n",
    "Before stopping the instance, remember to download the latest .ipynb file for the GitHub. Terminate the instance to delete all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = 'vgg-face-keras.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This network architecture is derived from Table 3 of the CNN described in Parkhi et al. \n",
    "# and based on Keras code provided in https://gist.github.com/EncodeTS/6bbe8cb8bebad7a672f0d872561782d9\n",
    "\n",
    "def vgg_face(weights_path=None):\n",
    "    img = Input(shape=(3, 224, 224))\n",
    "\n",
    "    pad1_1 = ZeroPadding2D(padding=(1, 1))(img)\n",
    "    conv1_1 = Convolution2D(64, (3, 3), activation='relu', name='conv1_1')(pad1_1)\n",
    "    pad1_2 = ZeroPadding2D(padding=(1, 1))(conv1_1)\n",
    "    conv1_2 = Convolution2D(64, (3, 3), activation='relu', name='conv1_2')(pad1_2)\n",
    "    pool1 = MaxPooling2D((2, 2), strides=(2, 2))(conv1_2)\n",
    "\n",
    "    pad2_1 = ZeroPadding2D((1, 1))(pool1)\n",
    "    conv2_1 = Convolution2D(128, (3, 3), activation='relu', name='conv2_1')(pad2_1)\n",
    "    pad2_2 = ZeroPadding2D((1, 1))(conv2_1)\n",
    "    conv2_2 = Convolution2D(128, (3, 3), activation='relu', name='conv2_2')(pad2_2)\n",
    "    pool2 = MaxPooling2D((2, 2), strides=(2, 2))(conv2_2)\n",
    "\n",
    "    pad3_1 = ZeroPadding2D((1, 1))(pool2)\n",
    "    conv3_1 = Convolution2D(256, (3, 3), activation='relu', name='conv3_1')(pad3_1)\n",
    "    pad3_2 = ZeroPadding2D((1, 1))(conv3_1)\n",
    "    conv3_2 = Convolution2D(256, (3, 3), activation='relu', name='conv3_2')(pad3_2)\n",
    "    pad3_3 = ZeroPadding2D((1, 1))(conv3_2)\n",
    "    conv3_3 = Convolution2D(256, (3, 3), activation='relu', name='conv3_3')(pad3_3)\n",
    "    pool3 = MaxPooling2D((2, 2), strides=(2, 2))(conv3_3)\n",
    "\n",
    "    pad4_1 = ZeroPadding2D((1, 1))(pool3)\n",
    "    conv4_1 = Convolution2D(512, (3, 3), activation='relu', name='conv4_1')(pad4_1)\n",
    "    pad4_2 = ZeroPadding2D((1, 1))(conv4_1)\n",
    "    conv4_2 = Convolution2D(512, (3, 3), activation='relu', name='conv4_2')(pad4_2)\n",
    "    pad4_3 = ZeroPadding2D((1, 1))(conv4_2)\n",
    "    conv4_3 = Convolution2D(512, (3, 3), activation='relu', name='conv4_3')(pad4_3)\n",
    "    pool4 = MaxPooling2D((2, 2), strides=(2, 2))(conv4_3)\n",
    "\n",
    "    pad5_1 = ZeroPadding2D((1, 1))(pool4)\n",
    "    conv5_1 = Convolution2D(512, (3, 3), activation='relu', name='conv5_1')(pad5_1)\n",
    "    pad5_2 = ZeroPadding2D((1, 1))(conv5_1)\n",
    "    conv5_2 = Convolution2D(512, (3, 3), activation='relu', name='conv5_2')(pad5_2)\n",
    "    pad5_3 = ZeroPadding2D((1, 1))(conv5_2)\n",
    "    conv5_3 = Convolution2D(512, (3, 3), activation='relu', name='conv5_3')(pad5_3)\n",
    "    pool5 = MaxPooling2D((2, 2), strides=(2, 2))(conv5_3)\n",
    "\n",
    "    # These layers are used in the original VGG Face paper for their dataset of 2,622 individuals\n",
    "    # The output of the previous layer is the 4096-dimensional face descriptor\n",
    "    fc6 = Convolution2D(4096, (7, 7), activation='relu', name='fc6')(pool5)\n",
    "    fc6_drop = Dropout(0.5)(fc6)\n",
    "    fc7 = Convolution2D(4096, (1, 1), activation='relu', name='fc7')(fc6_drop)\n",
    "    fc7_drop = Dropout(0.5)(fc7)\n",
    "    fc8 = Convolution2D(2622, (1, 1), name='fc8')(fc7_drop)\n",
    "    flat = Flatten()(fc8)\n",
    "    out = Activation('softmax')(flat)\n",
    "\n",
    "    model = Model(inputs=img, outputs=out)\n",
    "\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Returns model that for the 4096-dimensional face descriptor \n",
    "def partial_vgg_face():\n",
    "    model = vgg_face(weights_path)\n",
    "    layer_name = 'fc7'\n",
    "    partial_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer(layer_name).output)\n",
    "    return partial_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1, 3, 224, 224)\n",
      "0.999718\n"
     ]
    }
   ],
   "source": [
    "# Test the model by passing an image through it\n",
    "im = Image.open('A.J._Buckley.jpg')\n",
    "im = im.resize((224,224))\n",
    "im = np.array(im).astype(np.float32)\n",
    "# im[:,:,0] -= 129.1863\n",
    "# im[:,:,1] -= 104.7624\n",
    "# im[:,:,2] -= 93.5940\n",
    "im = im.transpose((2,0,1))\n",
    "im = np.expand_dims(im, axis=0)\n",
    "print('Shape:', im.shape)\n",
    "\n",
    "model = vgg_face(weights_path)\n",
    "out = model.predict(im)\n",
    "print(out[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4096, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Test the partial model by passing an image through it\n",
    "model = partial_vgg_face()\n",
    "im = Image.open('A.J._Buckley.jpg')\n",
    "im = im.resize((224,224))\n",
    "im = np.array(im).astype(np.float32)\n",
    "im = im.transpose((2,0,1))\n",
    "im = np.expand_dims(im, axis=0)\n",
    "\n",
    "descriptor = model.predict(im)\n",
    "print(descriptor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 3, 224, 224)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_14 (ZeroPaddi (None, 3, 226, 226)       0         \n",
      "_________________________________________________________________\n",
      "conv1_1 (Conv2D)             (None, 64, 224, 224)      1792      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_15 (ZeroPaddi (None, 64, 226, 226)      0         \n",
      "_________________________________________________________________\n",
      "conv1_2 (Conv2D)             (None, 64, 224, 224)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 64, 112, 112)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_16 (ZeroPaddi (None, 64, 114, 114)      0         \n",
      "_________________________________________________________________\n",
      "conv2_1 (Conv2D)             (None, 128, 112, 112)     73856     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_17 (ZeroPaddi (None, 128, 114, 114)     0         \n",
      "_________________________________________________________________\n",
      "conv2_2 (Conv2D)             (None, 128, 112, 112)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 128, 56, 56)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_18 (ZeroPaddi (None, 128, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv3_1 (Conv2D)             (None, 256, 56, 56)       295168    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_19 (ZeroPaddi (None, 256, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv3_2 (Conv2D)             (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_20 (ZeroPaddi (None, 256, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv3_3 (Conv2D)             (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 256, 28, 28)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_21 (ZeroPaddi (None, 256, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv4_1 (Conv2D)             (None, 512, 28, 28)       1180160   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_22 (ZeroPaddi (None, 512, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv4_2 (Conv2D)             (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_23 (ZeroPaddi (None, 512, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv4_3 (Conv2D)             (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 512, 14, 14)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_24 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv5_1 (Conv2D)             (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_25 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv5_2 (Conv2D)             (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_26 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv5_3 (Conv2D)             (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 512, 7, 7)         0         \n",
      "_________________________________________________________________\n",
      "fc6 (Conv2D)                 (None, 4096, 1, 1)        102764544 \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4096, 1, 1)        0         \n",
      "_________________________________________________________________\n",
      "fc7 (Conv2D)                 (None, 4096, 1, 1)        16781312  \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4096, 1, 1)        0         \n",
      "_________________________________________________________________\n",
      "fc8 (Conv2D)                 (None, 2622, 1, 1)        10742334  \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2622)              0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2622)              0         \n",
      "=================================================================\n",
      "Total params: 145,002,878\n",
      "Trainable params: 145,002,878\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "% load_ext autoreload\n",
    "% aimport experiment\n",
    "% aimport manipulations\n",
    "% autoreload 1\n",
    "\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "import manipulations\n",
    "import experiment\n",
    "from manipulations import ManipulationInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lfw_dataset(min_faces_per_person, manipulation_info):\n",
    "    dataset = fetch_lfw_people(\n",
    "        min_faces_per_person=min_faces_per_person, \n",
    "        color=True, \n",
    "        slice_=(slice(0, 250, None), slice(0, 250, None)), \n",
    "        resize=0.896)\n",
    "    data = dataset.images\n",
    "    # data = manipulations.perform_manipulation(data, manipulation_info)\n",
    "    # mean_face = np.mean(data, axis=0)\n",
    "    # data = data - mean_face\n",
    "\n",
    "    train_indices, test_indices = experiment.split_traintest(dataset.target)\n",
    "    train_data = data[train_indices,:]\n",
    "    train_targets = dataset.target[train_indices]\n",
    "    test_data = data[test_indices,:]\n",
    "    test_targets = dataset.target[test_indices]\n",
    "\n",
    "    # test_data = normalize(test_data, axis=1)\n",
    "    # train_data = normalize(train_data, axis=1)\n",
    "    # train_data, test_data, train_targets, test_targets = train_test_split(data, dataset.target)\n",
    "    \n",
    "    mean_face = [129.1863, 104.7624, 93.5940] # BGR\n",
    "    \n",
    "    train_data = train_data.transpose((0,3,1,2))\n",
    "    train_data[:,0,:,:] = train_data[:,0,:,:] - mean_face[0]\n",
    "    train_data[:,1,:,:] = train_data[:,1,:,:] - mean_face[1]\n",
    "    train_data[:,2,:,:] = train_data[:,2,:,:] - mean_face[2]\n",
    "#     train_data = train_data[:,::-1,:,:] # Flip to RGB? \n",
    "# Confusing because it seems like fetch_lfw_people() does some things to the original image \n",
    "# (coloring is off and the pixels are 0-255, not 0-1 as stated in documentation/their code)\n",
    "    \n",
    "    test_data = test_data.transpose((0,3,1,2))\n",
    "    test_data[:,0,:,:] = test_data[:,0,:,:] - mean_face[0]\n",
    "    test_data[:,1,:,:] = test_data[:,1,:,:] - mean_face[1]\n",
    "    test_data[:,2,:,:] = test_data[:,2,:,:] - mean_face[2]\n",
    "#     test_data = train_data[:,::-1,:,:] # Flip to RGB?\n",
    "    \n",
    "    return train_data, train_targets, test_data, test_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptors(model, data):    \n",
    "    descriptors = model.predict(data)\n",
    "    return descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(train, test):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "    print('Loading model')\n",
    "    model = partial_vgg_face()\n",
    "    \n",
    "    print('Loading dataset')\n",
    "    min_faces_per_person = 20\n",
    "    train_data, train_targets, test_data, test_targets = get_lfw_dataset(\n",
    "        min_faces_per_person, manipulation_info=ManipulationInfo(\"none\", {}))\n",
    "    \n",
    "    # Train\n",
    "    print('Testing')\n",
    "    time1 = time.clock()\n",
    "    train_descriptors = get_descriptors(model, train_data)\n",
    "    time2 = time.clock()\n",
    "    train_time = time2 - time1\n",
    "    \n",
    "    # Test\n",
    "    print('Testing')\n",
    "    time1 = time.clock()\n",
    "    test_descriptors = get_descriptors(model, test_data)\n",
    "    predictions = predict(train_descriptors, train_descriptors)\n",
    "    # Get accuracy\n",
    "    # Predict test_descriptors\n",
    "    time2 = time.clock()\n",
    "    train_time = time2 - time1\n",
    "    \n",
    "    # Print results.\n",
    "    num_faces = len(np.unique(train_targets))\n",
    "    print(\"Manipulation info: %s\" % str(manipulation_info))\n",
    "    print(\"Recognition Algorithm: %s\" % model_name)\n",
    "    print(\"Number of distinct faces: %d\" % num_faces)\n",
    "    print(\"Chance rate: %f\" % (1 / num_faces))\n",
    "    print(\"Train accuracy: %f\" % train_accuracy)\n",
    "    print(\"Test accuracy: %f\" % test_accuracy)\n",
    "    print(\"Training Time: %s sec\" % train_time)\n",
    "    print(\"Testing Time: %s sec\" % test_time)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    \n",
    "    return {\n",
    "        # \"Manipulation Type\": manipulation_info.type,\n",
    "        # \"Manipulation Parameters\": manipulation_info.parameters,\n",
    "        # \"Recognition Algorithm\": model_name,\n",
    "        \"Min Faces Per Person\": min_faces_per_person,\n",
    "        \"Number of Distinct Faces\": num_faces,\n",
    "        \"Chance Rate\": (1 / num_faces),\n",
    "        \"Train Accuracy\": train_accuracy,\n",
    "        \"Test Accuracy\": test_accuracy,\n",
    "        \"Training Time\": train_time,\n",
    "        \"Testing Time\": test_time,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = run_experiment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:theano_p36]",
   "language": "python",
   "name": "conda-env-theano_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
