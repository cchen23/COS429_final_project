{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COS 429 Final Project\n",
    "## VGG Face\n",
    "\n",
    "Initial setup:\n",
    "- Create instance (p2.xlarge)\n",
    "- `scp` the .caffemodel and .prototxt files over\n",
    "- Create ssl cert and password for Jupyter notebook\n",
    "\n",
    "To get this up and running on AWS (after initial setup):\n",
    "- `sudo ssh -i thesis.pem -L 443:127.0.0.1:8888 ubuntu@...`\n",
    "- `127.0.0.1`\n",
    "- Password: cos429_russakovsky\n",
    "- `source activate theano_p36`\n",
    "- `conda install -c anaconda pillow`\n",
    "- `conda install h5py`\n",
    "- `conda install scikit-learn`\n",
    "- `jupyter notebook`\n",
    "- `scp -i cos429.pem *.py ubuntu@...:~/cos429/`\n",
    "\n",
    "This uses the Keras weights (hard to get caffemodel and t7 files working for caffe2/pytorch) for VGG_FACE, which was converted from vgg-face matconvnet model using as shown here: https://gist.github.com/EncodeTS/6bbe8cb8bebad7a672f0d872561782d9.\n",
    "\n",
    "Before stopping the instance, remember to download the latest .ipynb file for the GitHub. Terminate the instance to delete all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys  \n",
    "os.environ['THEANO_FLAGS'] = \"device=gpu1\"    \n",
    "import theano\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = 'vgg-face-keras.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This network architecture is derived from Table 3 of the CNN described in Parkhi et al. \n",
    "# and based on Keras code provided in https://gist.github.com/EncodeTS/6bbe8cb8bebad7a672f0d872561782d9\n",
    "\n",
    "def vgg_face(weights_path=None):\n",
    "    img = Input(shape=(3, 224, 224))\n",
    "\n",
    "    pad1_1 = ZeroPadding2D(padding=(1, 1))(img)\n",
    "    conv1_1 = Convolution2D(64, (3, 3), activation='relu', name='conv1_1')(pad1_1)\n",
    "    pad1_2 = ZeroPadding2D(padding=(1, 1))(conv1_1)\n",
    "    conv1_2 = Convolution2D(64, (3, 3), activation='relu', name='conv1_2')(pad1_2)\n",
    "    pool1 = MaxPooling2D((2, 2), strides=(2, 2))(conv1_2)\n",
    "\n",
    "    pad2_1 = ZeroPadding2D((1, 1))(pool1)\n",
    "    conv2_1 = Convolution2D(128, (3, 3), activation='relu', name='conv2_1')(pad2_1)\n",
    "    pad2_2 = ZeroPadding2D((1, 1))(conv2_1)\n",
    "    conv2_2 = Convolution2D(128, (3, 3), activation='relu', name='conv2_2')(pad2_2)\n",
    "    pool2 = MaxPooling2D((2, 2), strides=(2, 2))(conv2_2)\n",
    "\n",
    "    pad3_1 = ZeroPadding2D((1, 1))(pool2)\n",
    "    conv3_1 = Convolution2D(256, (3, 3), activation='relu', name='conv3_1')(pad3_1)\n",
    "    pad3_2 = ZeroPadding2D((1, 1))(conv3_1)\n",
    "    conv3_2 = Convolution2D(256, (3, 3), activation='relu', name='conv3_2')(pad3_2)\n",
    "    pad3_3 = ZeroPadding2D((1, 1))(conv3_2)\n",
    "    conv3_3 = Convolution2D(256, (3, 3), activation='relu', name='conv3_3')(pad3_3)\n",
    "    pool3 = MaxPooling2D((2, 2), strides=(2, 2))(conv3_3)\n",
    "\n",
    "    pad4_1 = ZeroPadding2D((1, 1))(pool3)\n",
    "    conv4_1 = Convolution2D(512, (3, 3), activation='relu', name='conv4_1')(pad4_1)\n",
    "    pad4_2 = ZeroPadding2D((1, 1))(conv4_1)\n",
    "    conv4_2 = Convolution2D(512, (3, 3), activation='relu', name='conv4_2')(pad4_2)\n",
    "    pad4_3 = ZeroPadding2D((1, 1))(conv4_2)\n",
    "    conv4_3 = Convolution2D(512, (3, 3), activation='relu', name='conv4_3')(pad4_3)\n",
    "    pool4 = MaxPooling2D((2, 2), strides=(2, 2))(conv4_3)\n",
    "\n",
    "    pad5_1 = ZeroPadding2D((1, 1))(pool4)\n",
    "    conv5_1 = Convolution2D(512, (3, 3), activation='relu', name='conv5_1')(pad5_1)\n",
    "    pad5_2 = ZeroPadding2D((1, 1))(conv5_1)\n",
    "    conv5_2 = Convolution2D(512, (3, 3), activation='relu', name='conv5_2')(pad5_2)\n",
    "    pad5_3 = ZeroPadding2D((1, 1))(conv5_2)\n",
    "    conv5_3 = Convolution2D(512, (3, 3), activation='relu', name='conv5_3')(pad5_3)\n",
    "    pool5 = MaxPooling2D((2, 2), strides=(2, 2))(conv5_3)\n",
    "\n",
    "    # These layers are used in the original VGG Face paper for their dataset of 2,622 individuals\n",
    "    # The output of the previous layer is the 4096-dimensional face descriptor\n",
    "    fc6 = Convolution2D(4096, (7, 7), activation='relu', name='fc6')(pool5)\n",
    "    fc6_drop = Dropout(0.5)(fc6)\n",
    "    fc7 = Convolution2D(4096, (1, 1), activation='relu', name='fc7')(fc6_drop)\n",
    "    fc7_drop = Dropout(0.5)(fc7)\n",
    "    fc8 = Convolution2D(2622, (1, 1), name='fc8')(fc7_drop)\n",
    "    flat = Flatten()(fc8)\n",
    "    out = Activation('softmax')(flat)\n",
    "\n",
    "    model = Model(inputs=img, outputs=out)\n",
    "\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Returns model that for the 4096-dimensional face descriptor \n",
    "def partial_vgg_face():\n",
    "    model = vgg_face(weights_path)\n",
    "    layer_name = 'fc7'\n",
    "    partial_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer(layer_name).output)\n",
    "    return partial_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model by passing an image through it\n",
    "im = Image.open('A.J._Buckley.jpg')\n",
    "im = im.resize((224,224))\n",
    "im = np.array(im).astype(np.float32)\n",
    "# im[:,:,0] -= 129.1863\n",
    "# im[:,:,1] -= 104.7624\n",
    "# im[:,:,2] -= 93.5940\n",
    "im = im.transpose((2,0,1))\n",
    "im = np.expand_dims(im, axis=0)\n",
    "print('Shape:', im.shape)\n",
    "\n",
    "model = vgg_face(weights_path)\n",
    "out = model.predict(im)\n",
    "print(out[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the partial model by passing an image through it\n",
    "model = partial_vgg_face()\n",
    "im = Image.open('A.J._Buckley.jpg')\n",
    "im = im.resize((224,224))\n",
    "im = np.array(im).astype(np.float32)\n",
    "im = im.transpose((2,0,1))\n",
    "im = np.expand_dims(im, axis=0)\n",
    "\n",
    "descriptor = model.predict(im)\n",
    "print(descriptor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3, 224, 224)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 3, 226, 226)       0         \n",
      "_________________________________________________________________\n",
      "conv1_1 (Conv2D)             (None, 64, 224, 224)      1792      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 64, 226, 226)      0         \n",
      "_________________________________________________________________\n",
      "conv1_2 (Conv2D)             (None, 64, 224, 224)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 112, 112)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 64, 114, 114)      0         \n",
      "_________________________________________________________________\n",
      "conv2_1 (Conv2D)             (None, 128, 112, 112)     73856     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPaddin (None, 128, 114, 114)     0         \n",
      "_________________________________________________________________\n",
      "conv2_2 (Conv2D)             (None, 128, 112, 112)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 128, 56, 56)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPaddin (None, 128, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv3_1 (Conv2D)             (None, 256, 56, 56)       295168    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPaddin (None, 256, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv3_2 (Conv2D)             (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPaddin (None, 256, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv3_3 (Conv2D)             (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 256, 28, 28)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPaddin (None, 256, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv4_1 (Conv2D)             (None, 512, 28, 28)       1180160   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPaddin (None, 512, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv4_2 (Conv2D)             (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_10 (ZeroPaddi (None, 512, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv4_3 (Conv2D)             (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 512, 14, 14)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_11 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv5_1 (Conv2D)             (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_12 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv5_2 (Conv2D)             (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_13 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv5_3 (Conv2D)             (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 512, 7, 7)         0         \n",
      "_________________________________________________________________\n",
      "fc6 (Conv2D)                 (None, 4096, 1, 1)        102764544 \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096, 1, 1)        0         \n",
      "_________________________________________________________________\n",
      "fc7 (Conv2D)                 (None, 4096, 1, 1)        16781312  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096, 1, 1)        0         \n",
      "_________________________________________________________________\n",
      "fc8 (Conv2D)                 (None, 2622, 1, 1)        10742334  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2622)              0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2622)              0         \n",
      "=================================================================\n",
      "Total params: 145,002,878\n",
      "Trainable params: 145,002,878\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = vgg_face(weights_path)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "% load_ext autoreload\n",
    "% aimport experiment\n",
    "% aimport manipulations\n",
    "% autoreload 1\n",
    "\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from scipy import ndimage\n",
    "\n",
    "import manipulations\n",
    "import experiment\n",
    "from manipulations import ManipulationInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulations\n",
    "# Based on Cathy's manipulations.py\n",
    "\n",
    "def perform_manipulation(data, manipulation_info: ManipulationInfo):\n",
    "    manipulation_type = manipulation_info.type\n",
    "    manipulation_parameters = manipulation_info.parameters\n",
    "    if manipulation_type == \"none\":\n",
    "        return data\n",
    "    elif manipulation_type == \"occlude_lfw\":\n",
    "        occlusion_size = manipulation_parameters[\"occlusion_size\"]\n",
    "        return occlude_lfw_dataset(data, occlusion_size)\n",
    "    elif manipulation_type == \"radial_distortion\":\n",
    "        k = manipulation_parameters[\"k\"]\n",
    "        return radially_distort_lfw_dataset(data, k)\n",
    "    elif manipulation_type == \"blur\":\n",
    "        blurwindow_size = manipulation_parameters[\"blurwindow_size\"]\n",
    "        return blur_lfw_dataset(data, blurwindow_size)\n",
    "    else:\n",
    "        raise Exception(\"UNKNOWN MANIPULATION.\")\n",
    "        \n",
    "def occlude_lfw_dataset(data, occlusion_size):\n",
    "    num_images = data.shape[0]\n",
    "    dataset_images = [add_occlusion(data[i], occlusion_size) for i in range(num_images)]\n",
    "    return np.asarray(dataset_images)\n",
    "\n",
    "def add_occlusion(input_image, occlusion_size):\n",
    "    \"\"\"Randomly selects an occlusion_size-by-occlusion_size square in the image\n",
    "    and sets the pixels to random values between 0 and 256. Same value for all 3 channels.\"\"\"\n",
    "    max_value = 256\n",
    "    max_i, max_j, ch = input_image.shape\n",
    "    input_image = np.copy(input_image)\n",
    "    start_i = np.random.randint(0, max_i-occlusion_size)\n",
    "    start_j = np.random.randint(0, max_j-occlusion_size)\n",
    "    occlusion_square = np.random.rand(occlusion_size, occlusion_size)*max_value\n",
    "    input_image[start_i:start_i+occlusion_size,start_j:start_j+occlusion_size,0]=occlusion_square\n",
    "    input_image[start_i:start_i+occlusion_size,start_j:start_j+occlusion_size,1]=occlusion_square\n",
    "    input_image[start_i:start_i+occlusion_size,start_j:start_j+occlusion_size,2]=occlusion_square\n",
    "    return input_image\n",
    "\n",
    "def radially_distort_lfw_dataset(data, k):\n",
    "    lfw_imageshape = (224, 224)\n",
    "    distortion_array_i, distortion_array_j = create_radial_distortion_array(k, lfw_imageshape)\n",
    "    num_images = data.shape[0]\n",
    "    dataset_images = [radial_distortion(data[i], distortion_array_i, distortion_array_j) for i in range(num_images)]\n",
    "    return np.asarray(dataset_images)\n",
    "\n",
    "def radial_distortion(input_image, distortion_array_i, distortion_array_j):\n",
    "    distorted_image = np.empty(input_image.shape)\n",
    "    for i in range(distorted_image.shape[0]):\n",
    "        for j in range(distorted_image.shape[1]):\n",
    "            input_i = distortion_array_i[i, j]\n",
    "            input_j = distortion_array_j[i, j]\n",
    "            distorted_image[i,j,0] = input_image[input_i, input_j,0]\n",
    "            distorted_image[i,j,1] = input_image[input_i, input_j,1]\n",
    "            distorted_image[i,j,2] = input_image[input_i, input_j,2]\n",
    "    return distorted_image\n",
    "\n",
    "def create_radial_distortion_array(k, input_image_shape):\n",
    "    # http://sprg.massey.ac.nz/pdfs/2003_IVCNZ_408.pdf\n",
    "    # x_d = x_u / (1+kr_d^2)\n",
    "    # Negative k for pincushion, positive k for barrel.\n",
    "    i_max, j_max = input_image_shape\n",
    "    i0 = int(i_max / 2)\n",
    "    j0 = int(j_max / 2)\n",
    "    distortion_array_i = np.zeros(input_image_shape, dtype='int')\n",
    "    distortion_array_j = np.zeros(input_image_shape, dtype='int')\n",
    "    for i in range(i_max):\n",
    "        for j in range(j_max):\n",
    "            i_bar = i - i0\n",
    "            j_bar = j - j0\n",
    "            r_squared = (i_bar * i_bar) + (j_bar * j_bar)\n",
    "            i_input = i / (1+k*r_squared)\n",
    "            j_input = j / (1+k*r_squared)\n",
    "            if i_input < i_max and j_input < j_max and i_input >= 0 and j_input >= 0:\n",
    "                distortion_array_i[i, j] = i_input\n",
    "                distortion_array_j[i, j] = j_input\n",
    "    return distortion_array_i, distortion_array_j\n",
    "\n",
    "def blur_lfw_dataset(data, blurwindow_size):\n",
    "    num_images = data.shape[0]\n",
    "    dataset_images = [blur(data[i], blurwindow_size) for i in range(num_images)]\n",
    "    return np.asarray(dataset_images)\n",
    "\n",
    "def blur(input_image, blurwindow_size):\n",
    "    blurred_image = np.empty(input_image.shape)\n",
    "    blurred_image[:,:,0] = ndimage.percentile_filter(input_image[:,:,0], -50, blurwindow_size)\n",
    "    blurred_image[:,:,1] = ndimage.percentile_filter(input_image[:,:,1], -50, blurwindow_size)\n",
    "    blurred_image[:,:,2] = ndimage.percentile_filter(input_image[:,:,2], -50, blurwindow_size)\n",
    "    return blurred_image\n",
    "\n",
    "def blur_slow(input_image, blurwindow_size):\n",
    "    blurred_image = np.empty(input_image.shape)\n",
    "    blurwindow_halflength = blurwindow_size / 2\n",
    "    image_imax, image_jmax = input_image.shape\n",
    "    for i in range(image_imax):\n",
    "        for j in range(image_jmax):\n",
    "            blurwindow_imin = int(max(0, i-blurwindow_halflength))\n",
    "            blurwindow_jmin = int(max(0, j-blurwindow_halflength))\n",
    "            blurwindow_imax = int(min(image_imax, i+blurwindow_halflength))\n",
    "            blurwindow_jmax = int(min(image_jmax, j+blurwindow_halflength))\n",
    "            blurred_image[i,j,0] = np.mean(input_image[blurwindow_imin:blurwindow_imax,blurwindow_jmin:blurwindow_jmax,0])\n",
    "            blurred_image[i,j,1] = np.mean(input_image[blurwindow_imin:blurwindow_imax,blurwindow_jmin:blurwindow_jmax,1])\n",
    "            blurred_image[i,j,2] = np.mean(input_image[blurwindow_imin:blurwindow_imax,blurwindow_jmin:blurwindow_jmax,2])\n",
    "    return blurred_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test manipulations\n",
    "\n",
    "im = Image.open('A.J._Buckley.jpg')\n",
    "im = im.resize((224,224))\n",
    "im = np.array(im).astype(np.float32)\n",
    "im = np.expand_dims(im, axis=0)\n",
    "\n",
    "#         ManipulationInfo(\"none\", {}),\n",
    "#         ManipulationInfo(\"occlude_lfw\", {\"occlusion_size\": 20}),\n",
    "#         ManipulationInfo(\"occlude_lfw\", {\"occlusion_size\": 10}),\n",
    "#         ManipulationInfo(\"occlude_lfw\", {\"occlusion_size\": 30}),\n",
    "#         ManipulationInfo(\"occlude_lfw\", {\"occlusion_size\": 40}),\n",
    "#         ManipulationInfo(\"radial_distortion\", {\"k\": 0.00015}),\n",
    "#         ManipulationInfo(\"radial_distortion\", {\"k\": -0.00015}),\n",
    "#         ManipulationInfo(\"radial_distortion\", {\"k\": 0.0003}),\n",
    "#         ManipulationInfo(\"radial_distortion\", {\"k\": -0.0003}),\n",
    "#         ManipulationInfo(\"radial_distortion\", {\"k\": 0.0005}),\n",
    "#         ManipulationInfo(\"radial_distortion\", {\"k\": -0.0005}),\n",
    "#         ManipulationInfo(\"blur\", {\"blurwindow_size\": 5}),\n",
    "#         ManipulationInfo(\"blur\", {\"blurwindow_size\": 10})\n",
    "\n",
    "imM = perform_manipulation(im, ManipulationInfo(\"radial_distortion\", {\"k\": 0.00003}))\n",
    "plt.imshow(imM[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get LFW dataset\n",
    "\n",
    "def get_lfw_dataset(min_faces_per_person, manipulation_info: ManipulationInfo):\n",
    "    dataset = fetch_lfw_people(\n",
    "        min_faces_per_person=min_faces_per_person, \n",
    "        color=True, \n",
    "        slice_=(slice(0, 250, None), slice(0, 250, None)), \n",
    "        resize=0.896)\n",
    "    data = dataset.images\n",
    "    # data = manipulations.perform_manipulation(data, manipulation_info)\n",
    "    # mean_face = np.mean(data, axis=0)\n",
    "    # data = data - mean_face\n",
    "\n",
    "    train_indices, test_indices = experiment.split_traintest(dataset.target)\n",
    "    train_data = data[train_indices,:]\n",
    "    train_targets = dataset.target[train_indices]\n",
    "    test_data = perform_manipulation(data[test_indices,:], manipulation_info)\n",
    "    test_targets = dataset.target[test_indices]\n",
    "\n",
    "    # test_data = normalize(test_data, axis=1)\n",
    "    # train_data = normalize(train_data, axis=1)\n",
    "    # train_data, test_data, train_targets, test_targets = train_test_split(data, dataset.target)\n",
    "    \n",
    "    mean_face = [129.1863, 104.7624, 93.5940] # BGR\n",
    "    \n",
    "    train_data = train_data.transpose((0,3,1,2))\n",
    "    train_data[:,0,:,:] = train_data[:,0,:,:] - mean_face[0]\n",
    "    train_data[:,1,:,:] = train_data[:,1,:,:] - mean_face[1]\n",
    "    train_data[:,2,:,:] = train_data[:,2,:,:] - mean_face[2]\n",
    "#     train_data = train_data[:,::-1,:,:] # Flip to RGB? \n",
    "# Confusing because it seems like fetch_lfw_people() does some things to the original image \n",
    "# (coloring is off and the pixels are 0-255, not 0-1 as stated in documentation/their code)\n",
    "    \n",
    "    test_data = test_data.transpose((0,3,1,2))\n",
    "    test_data[:,0,:,:] = test_data[:,0,:,:] - mean_face[0]\n",
    "    test_data[:,1,:,:] = test_data[:,1,:,:] - mean_face[1]\n",
    "    test_data[:,2,:,:] = test_data[:,2,:,:] - mean_face[2]\n",
    "#     test_data = train_data[:,::-1,:,:] # Flip to RGB?\n",
    "    \n",
    "    return train_data, train_targets, test_data, test_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptors(model, data):    \n",
    "    descriptors = model.predict(data, verbose=1)\n",
    "    return np.squeeze(descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(mean_descriptors, descriptors, threshold=None):\n",
    "    predictions = []\n",
    "    for d in descriptors:\n",
    "        distances = [np.linalg.norm(mean_descriptors[i] - d) for i in range(len(mean_descriptors))]\n",
    "        predictions.append(np.argmin(distances))\n",
    "    return np.asarray(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parkhi's paper \"learns\" a threshold value to determine whether f1 and f2 have the same identity\n",
    "# We technically just need to find the face pairs with the smallest distance?\n",
    "def find_threshold(mean_descriptors, descriptors):\n",
    "    num_faces = len(mean_descriptors)\n",
    "    num_examples_per_face = int(len(descriptors) / num_faces)\n",
    "    \n",
    "    distances = [np.linalg.norm(mean_descriptors[int(i/3)] - descriptors[i]) for i in range(len(descriptors))]\n",
    "    threshold = max(distances)\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(manipulation_info: ManipulationInfo):\n",
    "    print('Loading model')\n",
    "    model = partial_vgg_face()\n",
    "    \n",
    "    print('Loading dataset')\n",
    "    min_faces_per_person = 20\n",
    "    train_data, train_targets, test_data, test_targets = get_lfw_dataset(\n",
    "        min_faces_per_person, manipulation_info=manipulation_info)\n",
    "    \n",
    "    # Train\n",
    "    print('Training')\n",
    "    time1 = time.clock()\n",
    "    num_faces = len(np.unique(train_targets))\n",
    "    num_examples_per_face = int(len(train_targets) / num_faces)\n",
    "    train_descriptors = get_descriptors(model, train_data)\n",
    "    test_descriptors = get_descriptors(model, test_data)\n",
    "    mean_train_descriptors = np.mean(np.reshape(train_descriptors, (-1, num_examples_per_face, 4096)), axis=1)\n",
    "    # threshold = find_threshold(mean_train_descriptors, train_descriptors)\n",
    "    time2 = time.clock()\n",
    "    train_time = time2 - time1\n",
    "    \n",
    "    # Test\n",
    "    print('Testing')\n",
    "    time1 = time.clock()\n",
    "    train_predictions = predict(mean_train_descriptors, train_descriptors)\n",
    "    train_accuracy = experiment.compute_accuracy(train_predictions, train_targets)\n",
    "    # Predict test_descriptors\n",
    "    test_predictions = predict(mean_train_descriptors, test_descriptors)\n",
    "    test_accuracy = experiment.compute_accuracy(test_predictions, test_targets)\n",
    "    time2 = time.clock()\n",
    "    test_time = time2 - time1\n",
    "    \n",
    "    # Print results.\n",
    "    num_faces = len(np.unique(train_targets))\n",
    "    model_name = 'VGG_FACE'\n",
    "    print(\"Manipulation info: %s\" % str(manipulation_info))\n",
    "    print(\"Recognition Algorithm: %s\" % model_name)\n",
    "    print(\"Number of distinct faces: %d\" % num_faces)\n",
    "    print(\"Chance rate: %f\" % (1 / num_faces))\n",
    "    print(\"Train accuracy: %f\" % train_accuracy)\n",
    "    print(\"Test accuracy: %f\" % test_accuracy)\n",
    "    print(\"Training Time: %s sec\" % train_time)\n",
    "    print(\"Testing Time: %s sec\" % test_time)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    \n",
    "    return {\n",
    "        \"Manipulation Type\": manipulation_info.type,\n",
    "        \"Manipulation Parameters\": manipulation_info.parameters,\n",
    "        \"Recognition Algorithm\": model_name,\n",
    "        \"Min Faces Per Person\": min_faces_per_person,\n",
    "        \"Number of Distinct Faces\": num_faces,\n",
    "        \"Chance Rate\": (1 / num_faces),\n",
    "        \"Train Accuracy\": train_accuracy,\n",
    "        \"Test Accuracy\": test_accuracy,\n",
    "        \"Training Time\": train_time,\n",
    "        \"Testing Time\": test_time,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n",
      "Loading dataset\n",
      "Training\n",
      "186/186 [==============================] - 76s 407ms/step\n",
      "2837/2837 [==============================] - 1150s 406ms/step\n",
      "Testing\n",
      "Manipulation info: ManipulationInfo(type='occlude_lfw', parameters={'occlusion_size': 82})\n",
      "Recognition Algorithm: VGG_FACE\n",
      "Number of distinct faces: 62\n",
      "Chance rate: 0.016129\n",
      "Train accuracy: 1.000000\n",
      "Test accuracy: 0.406768\n",
      "Training Time: 2427.5211830000003 sec\n",
      "Testing Time: 1.958480999999665 sec\n",
      "\n",
      "\n",
      "{'Manipulation Type': 'occlude_lfw', 'Manipulation Parameters': {'occlusion_size': 82}, 'Recognition Algorithm': 'VGG_FACE', 'Min Faces Per Person': 20, 'Number of Distinct Faces': 62, 'Chance Rate': 0.016129032258064516, 'Train Accuracy': 1.0, 'Test Accuracy': 0.4067677123722242, 'Training Time': 2427.5211830000003, 'Testing Time': 1.958480999999665}\n",
      "\n",
      "Loading model\n",
      "Loading dataset\n",
      "Training\n",
      "186/186 [==============================] - 76s 408ms/step\n",
      "2837/2837 [==============================] - 1151s 406ms/step\n",
      "Testing\n",
      "Manipulation info: ManipulationInfo(type='occlude_lfw', parameters={'occlusion_size': 41})\n",
      "Recognition Algorithm: VGG_FACE\n",
      "Number of distinct faces: 62\n",
      "Chance rate: 0.016129\n",
      "Train accuracy: 1.000000\n",
      "Test accuracy: 0.773000\n",
      "Training Time: 2430.146885 sec\n",
      "Testing Time: 1.8458709999995335 sec\n",
      "\n",
      "\n",
      "{'Manipulation Type': 'occlude_lfw', 'Manipulation Parameters': {'occlusion_size': 41}, 'Recognition Algorithm': 'VGG_FACE', 'Min Faces Per Person': 20, 'Number of Distinct Faces': 62, 'Chance Rate': 0.016129032258064516, 'Train Accuracy': 1.0, 'Test Accuracy': 0.77299964751498063, 'Training Time': 2430.146885, 'Testing Time': 1.8458709999995335}\n",
      "\n",
      "Loading model\n",
      "Loading dataset\n",
      "Training\n",
      "186/186 [==============================] - 76s 407ms/step\n",
      "2837/2837 [==============================] - 1152s 406ms/step\n",
      "Testing\n",
      "Manipulation info: ManipulationInfo(type='occlude_lfw', parameters={'occlusion_size': 124})\n",
      "Recognition Algorithm: VGG_FACE\n",
      "Number of distinct faces: 62\n",
      "Chance rate: 0.016129\n",
      "Train accuracy: 1.000000\n",
      "Test accuracy: 0.024674\n",
      "Training Time: 2430.8892380000007 sec\n",
      "Testing Time: 1.9282860000002984 sec\n",
      "\n",
      "\n",
      "{'Manipulation Type': 'occlude_lfw', 'Manipulation Parameters': {'occlusion_size': 124}, 'Recognition Algorithm': 'VGG_FACE', 'Min Faces Per Person': 20, 'Number of Distinct Faces': 62, 'Chance Rate': 0.016129032258064516, 'Train Accuracy': 1.0, 'Test Accuracy': 0.024673951357067323, 'Training Time': 2430.8892380000007, 'Testing Time': 1.9282860000002984}\n",
      "\n",
      "Loading model\n",
      "Loading dataset\n",
      "Training\n",
      "186/186 [==============================] - 76s 410ms/step\n",
      "2837/2837 [==============================] - 1151s 406ms/step\n",
      "Testing\n",
      "Manipulation info: ManipulationInfo(type='occlude_lfw', parameters={'occlusion_size': 165})\n",
      "Recognition Algorithm: VGG_FACE\n",
      "Number of distinct faces: 62\n",
      "Chance rate: 0.016129\n",
      "Train accuracy: 1.000000\n",
      "Test accuracy: 0.013394\n",
      "Training Time: 2429.647927 sec\n",
      "Testing Time: 1.9999709999992774 sec\n",
      "\n",
      "\n",
      "{'Manipulation Type': 'occlude_lfw', 'Manipulation Parameters': {'occlusion_size': 165}, 'Recognition Algorithm': 'VGG_FACE', 'Min Faces Per Person': 20, 'Number of Distinct Faces': 62, 'Chance Rate': 0.016129032258064516, 'Train Accuracy': 1.0, 'Test Accuracy': 0.013394430736693691, 'Training Time': 2429.647927, 'Testing Time': 1.9999709999992774}\n",
      "\n",
      "Loading model\n",
      "Loading dataset\n",
      "Training\n",
      "186/186 [==============================] - 75s 405ms/step\n",
      "2837/2837 [==============================] - 1149s 405ms/step\n",
      "Testing\n",
      "Manipulation info: ManipulationInfo(type='radial_distortion', parameters={'k': 8e-06})\n",
      "Recognition Algorithm: VGG_FACE\n",
      "Number of distinct faces: 62\n",
      "Chance rate: 0.016129\n",
      "Train accuracy: 1.000000\n",
      "Test accuracy: 0.864646\n",
      "Training Time: 2421.536619999999 sec\n",
      "Testing Time: 1.8982529999993858 sec\n",
      "\n",
      "\n",
      "{'Manipulation Type': 'radial_distortion', 'Manipulation Parameters': {'k': 8e-06}, 'Recognition Algorithm': 'VGG_FACE', 'Min Faces Per Person': 20, 'Number of Distinct Faces': 62, 'Chance Rate': 0.016129032258064516, 'Train Accuracy': 1.0, 'Test Accuracy': 0.86464575255551634, 'Training Time': 2421.536619999999, 'Testing Time': 1.8982529999993858}\n",
      "\n",
      "Loading model\n",
      "Loading dataset\n",
      "Training\n",
      "186/186 [==============================] - 75s 406ms/step\n",
      "2837/2837 [==============================] - 1148s 405ms/step\n",
      "Testing\n",
      "Manipulation info: ManipulationInfo(type='radial_distortion', parameters={'k': -8e-06})\n",
      "Recognition Algorithm: VGG_FACE\n",
      "Number of distinct faces: 62\n",
      "Chance rate: 0.016129\n",
      "Train accuracy: 1.000000\n",
      "Test accuracy: 0.797321\n",
      "Training Time: 2421.5872710000003 sec\n",
      "Testing Time: 1.8908579999988433 sec\n",
      "\n",
      "\n",
      "{'Manipulation Type': 'radial_distortion', 'Manipulation Parameters': {'k': -8e-06}, 'Recognition Algorithm': 'VGG_FACE', 'Min Faces Per Person': 20, 'Number of Distinct Faces': 62, 'Chance Rate': 0.016129032258064516, 'Train Accuracy': 1.0, 'Test Accuracy': 0.79732111385266125, 'Training Time': 2421.5872710000003, 'Testing Time': 1.8908579999988433}\n",
      "\n",
      "Loading model\n",
      "Loading dataset\n",
      "Training\n",
      "186/186 [==============================] - 75s 402ms/step\n",
      "2837/2837 [==============================] - 1152s 406ms/step\n",
      "Testing\n",
      "Manipulation info: ManipulationInfo(type='radial_distortion', parameters={'k': 1.6e-05})\n",
      "Recognition Algorithm: VGG_FACE\n",
      "Number of distinct faces: 62\n",
      "Chance rate: 0.016129\n",
      "Train accuracy: 1.000000\n",
      "Test accuracy: 0.818118\n",
      "Training Time: 2427.3962950000023 sec\n",
      "Testing Time: 1.9219909999992524 sec\n",
      "\n",
      "\n",
      "{'Manipulation Type': 'radial_distortion', 'Manipulation Parameters': {'k': 1.6e-05}, 'Recognition Algorithm': 'VGG_FACE', 'Min Faces Per Person': 20, 'Number of Distinct Faces': 62, 'Chance Rate': 0.016129032258064516, 'Train Accuracy': 1.0, 'Test Accuracy': 0.81811772999647514, 'Training Time': 2427.3962950000023, 'Testing Time': 1.9219909999992524}\n",
      "\n",
      "Loading model\n",
      "Loading dataset\n",
      "Training\n",
      "186/186 [==============================] - 75s 402ms/step\n",
      "2837/2837 [==============================] - 1135s 400ms/step\n",
      "Testing\n",
      "Manipulation info: ManipulationInfo(type='radial_distortion', parameters={'k': -1.6e-05})\n",
      "Recognition Algorithm: VGG_FACE\n",
      "Number of distinct faces: 62\n",
      "Chance rate: 0.016129\n",
      "Train accuracy: 1.000000\n",
      "Test accuracy: 0.708495\n",
      "Training Time: 2395.2620760000027 sec\n",
      "Testing Time: 1.9583650000022317 sec\n",
      "\n",
      "\n",
      "{'Manipulation Type': 'radial_distortion', 'Manipulation Parameters': {'k': -1.6e-05}, 'Recognition Algorithm': 'VGG_FACE', 'Min Faces Per Person': 20, 'Number of Distinct Faces': 62, 'Chance Rate': 0.016129032258064516, 'Train Accuracy': 1.0, 'Test Accuracy': 0.70849488896721891, 'Training Time': 2395.2620760000027, 'Testing Time': 1.9583650000022317}\n",
      "\n",
      "Loading model\n",
      "Loading dataset\n",
      "Training\n",
      "186/186 [==============================] - 79s 423ms/step\n",
      "2837/2837 [==============================] - 1144s 403ms/step\n",
      "Testing\n",
      "Manipulation info: ManipulationInfo(type='radial_distortion', parameters={'k': 3e-05})\n",
      "Recognition Algorithm: VGG_FACE\n",
      "Number of distinct faces: 62\n",
      "Chance rate: 0.016129\n",
      "Train accuracy: 1.000000\n",
      "Test accuracy: 0.625308\n",
      "Training Time: 2419.4993380000014 sec\n",
      "Testing Time: 1.8946369999976014 sec\n",
      "\n",
      "\n",
      "{'Manipulation Type': 'radial_distortion', 'Manipulation Parameters': {'k': 3e-05}, 'Recognition Algorithm': 'VGG_FACE', 'Min Faces Per Person': 20, 'Number of Distinct Faces': 62, 'Chance Rate': 0.016129032258064516, 'Train Accuracy': 1.0, 'Test Accuracy': 0.62530842439196332, 'Training Time': 2419.4993380000014, 'Testing Time': 1.8946369999976014}\n",
      "\n",
      "Loading model\n",
      "Loading dataset\n",
      "Training\n",
      "186/186 [==============================] - 74s 399ms/step\n",
      "2837/2837 [==============================] - 1138s 401ms/step\n",
      "Testing\n",
      "Manipulation info: ManipulationInfo(type='radial_distortion', parameters={'k': -3e-05})\n",
      "Recognition Algorithm: VGG_FACE\n",
      "Number of distinct faces: 62\n",
      "Chance rate: 0.016129\n",
      "Train accuracy: 1.000000\n",
      "Test accuracy: 0.507931\n",
      "Training Time: 2399.7880049999985 sec\n",
      "Testing Time: 1.9711200000019744 sec\n",
      "\n",
      "\n",
      "{'Manipulation Type': 'radial_distortion', 'Manipulation Parameters': {'k': -3e-05}, 'Recognition Algorithm': 'VGG_FACE', 'Min Faces Per Person': 20, 'Number of Distinct Faces': 62, 'Chance Rate': 0.016129032258064516, 'Train Accuracy': 1.0, 'Test Accuracy': 0.50793091293620019, 'Training Time': 2399.7880049999985, 'Testing Time': 1.9711200000019744}\n",
      "\n",
      "Loading model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "Training\n",
      "186/186 [==============================] - 75s 401ms/step\n",
      "2837/2837 [==============================] - 1140s 402ms/step\n",
      "Testing\n",
      "Manipulation info: ManipulationInfo(type='blur', parameters={'blurwindow_size': 5})\n",
      "Recognition Algorithm: VGG_FACE\n",
      "Number of distinct faces: 62\n",
      "Chance rate: 0.016129\n",
      "Train accuracy: 1.000000\n",
      "Test accuracy: 0.816003\n",
      "Training Time: 2405.032998999999 sec\n",
      "Testing Time: 1.8945559999992838 sec\n",
      "\n",
      "\n",
      "{'Manipulation Type': 'blur', 'Manipulation Parameters': {'blurwindow_size': 5}, 'Recognition Algorithm': 'VGG_FACE', 'Min Faces Per Person': 20, 'Number of Distinct Faces': 62, 'Chance Rate': 0.016129032258064516, 'Train Accuracy': 1.0, 'Test Accuracy': 0.81600281988015511, 'Training Time': 2405.032998999999, 'Testing Time': 1.8945559999992838}\n",
      "\n",
      "Loading model\n",
      "Loading dataset\n",
      "Training\n",
      "186/186 [==============================] - 75s 402ms/step\n",
      "2837/2837 [==============================] - 1141s 402ms/step\n",
      "Testing\n",
      "Manipulation info: ManipulationInfo(type='blur', parameters={'blurwindow_size': 10})\n",
      "Recognition Algorithm: VGG_FACE\n",
      "Number of distinct faces: 62\n",
      "Chance rate: 0.016129\n",
      "Train accuracy: 1.000000\n",
      "Test accuracy: 0.515686\n",
      "Training Time: 2405.975219 sec\n",
      "Testing Time: 1.948157999999239 sec\n",
      "\n",
      "\n",
      "{'Manipulation Type': 'blur', 'Manipulation Parameters': {'blurwindow_size': 10}, 'Recognition Algorithm': 'VGG_FACE', 'Min Faces Per Person': 20, 'Number of Distinct Faces': 62, 'Chance Rate': 0.016129032258064516, 'Train Accuracy': 1.0, 'Test Accuracy': 0.51568558336270709, 'Training Time': 2405.975219, 'Testing Time': 1.948157999999239}\n",
      "\n",
      "Loading model\n",
      "Loading dataset\n",
      "Training\n",
      "186/186 [==============================] - 75s 401ms/step\n",
      "2837/2837 [==============================] - 1141s 402ms/step\n",
      "Testing\n",
      "Manipulation info: ManipulationInfo(type='none', parameters={})\n",
      "Recognition Algorithm: VGG_FACE\n",
      "Number of distinct faces: 62\n",
      "Chance rate: 0.016129\n",
      "Train accuracy: 1.000000\n",
      "Test accuracy: 0.854424\n",
      "Training Time: 2407.0715129999953 sec\n",
      "Testing Time: 2.0076699999990524 sec\n",
      "\n",
      "\n",
      "{'Manipulation Type': 'none', 'Manipulation Parameters': {}, 'Recognition Algorithm': 'VGG_FACE', 'Min Faces Per Person': 20, 'Number of Distinct Faces': 62, 'Chance Rate': 0.016129032258064516, 'Train Accuracy': 1.0, 'Test Accuracy': 0.85442368699330273, 'Training Time': 2407.0715129999953, 'Testing Time': 2.0076699999990524}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "manipulation_infos = [\n",
    "        # ManipulationInfo(\"none\", {}),\n",
    "        ManipulationInfo(\"occlude_lfw\", {\"occlusion_size\": 82}), # Adjusted for new image size\n",
    "        ManipulationInfo(\"occlude_lfw\", {\"occlusion_size\": 41}),\n",
    "        ManipulationInfo(\"occlude_lfw\", {\"occlusion_size\": 124}),\n",
    "        ManipulationInfo(\"occlude_lfw\", {\"occlusion_size\": 165}),\n",
    "        ManipulationInfo(\"radial_distortion\", {\"k\": 0.000008}), # Adjusted for new image size\n",
    "        ManipulationInfo(\"radial_distortion\", {\"k\": -0.000008}),\n",
    "        ManipulationInfo(\"radial_distortion\", {\"k\": 0.000016}),\n",
    "        ManipulationInfo(\"radial_distortion\", {\"k\": -0.000016}),\n",
    "        ManipulationInfo(\"radial_distortion\", {\"k\": 0.00003}),\n",
    "        ManipulationInfo(\"radial_distortion\", {\"k\": -0.00003}),\n",
    "        ManipulationInfo(\"blur\", {\"blurwindow_size\": 5}),\n",
    "        ManipulationInfo(\"blur\", {\"blurwindow_size\": 10}),\n",
    "        ManipulationInfo(\"none\", {})\n",
    "    ]\n",
    "\n",
    "for manipulation in manipulation_infos:\n",
    "    stats = run_experiment(manipulation)\n",
    "    print(stats)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:theano_p36]",
   "language": "python",
   "name": "conda-env-theano_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
