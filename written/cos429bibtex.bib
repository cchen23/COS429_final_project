@inproceedings{sklearn_api,
  author    = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and
               Fabian Pedregosa and Andreas Mueller and Olivier Grisel and
               Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort
               and Jaques Grobler and Robert Layton and Jake VanderPlas and
               Arnaud Joly and Brian Holt and Ga{\"{e}}l Varoquaux},
  title     = {{API} design for machine learning software: experiences from the scikit-learn
               project},
  booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
  year      = {2013},
  pages = {108--122},
}

@article{russell_super-recognizers:_2009,
	title = {Super-recognizers: {People} with extraordinary face recognition ability},
	volume = {16},
	issn = {1069-9384, 1531-5320},
	shorttitle = {Super-recognizers},
	url = {https://link.springer.com/article/10.3758/PBR.16.2.252},
	doi = {10.3758/PBR.16.2.252},
	abstract = {We tested 4 people who claimed to have significantly better than ordinary face recognition ability. Exceptional ability was confirmed in each case. On two very different tests of face recognition, all 4 experimental subjects performed beyond the range of control subject performance. They also scored significantly better than average on a perceptual discrimination test with faces. This effect was larger with upright than with inverted faces, and the 4 subjects showed a larger “inversion effect” than did control subjects, who in turn showed a larger inversion effect than did developmental prosopagnosics. This result indicates an association between face recognition ability and the magnitude of the inversion effect. Overall, these “super-recognizers” are about as good at face recognition and perception as developmental prosopagnosics are bad. Our findings demonstrate the existence of people with exceptionally good face recognition ability and show that the range of face recognition and face perception ability is wider than has been previously acknowledged.},
	language = {en},
	number = {2},
	urldate = {2017-12-11},
	journal = {Psychonomic Bulletin \& Review},
	author = {Russell, Richard and Duchaine, Brad and Nakayama, Ken},
	month = apr,
	year = {2009},
	pages = {252--257},
	file = {Full Text PDF:C\:\\Users\\Cathy\\Zotero\\storage\\BI84NR5M\\Russell et al. - 2009 - Super-recognizers People with extraordinary face .pdf:application/pdf;Snapshot:C\:\\Users\\Cathy\\Zotero\\storage\\AMNV9CF5\\PBR.16.2.html:text/html}
}

@misc{flanagan_color_2011,
	title = {color {FERET} {Database}},
	url = {https://www.nist.gov/itl/iad/image-group/color-feret-database},
	abstract = {Version 2 The DOD Counterdrug Technology Program sponsored the Facial Recognition Technology (FERET) program and development of the FERET database. The National Institute of Standards and Technology (NIST) is serving as Technical Agent for distribution of the FERET database. The goal of the FERET program is to develop new techniques, technology, and algorithms for the},
	urldate = {2017-12-11},
	journal = {NIST},
	author = {Flanagan, Patricia A.},
	month = jan,
	year = {2011}
}

@techreport{huang_labeled_2007,
	title = {Labeled {Faces} in the {Wild}: {A} {Database} for {Studying} {Face} {Recognition} in {Unconstrained} {Environments}},
	url = {http://vis-www.cs.umass.edu/lfw/},
	number = {07-49},
	urldate = {2017-12-11},
	institution = {University of Massachusetts, Amherst},
	author = {Huang, Gary and Ramesh, Manu and Berg, Tamara and Learned-Miller, Erik},
	month = oct,
	year = {2007},
	file = {LFW Face Database \: Main:C\:\\Users\\Cathy\\Zotero\\storage\\Z3HZC5PY\\lfw.html:text/html}
}

@misc{noauthor_cartoonfaces_nodate,
	title = {{cartoonFaces}},
	url = {http://cvit.iiit.ac.in/research/projects/cvit-projects/cartoonfaces},
	urldate = {2017-12-11},
	file = {cartoonFaces:C\:\\Users\\Cathy\\Zotero\\storage\\R2GZEF9U\\cartoonfaces.html:text/html}
}

@misc{noauthor_face_nodate,
	title = {Face {Recognition} {Homepage} - {Algorithms}},
	url = {http://www.face-rec.org/algorithms/},
	urldate = {2017-12-11},
	file = {Face Recognition Homepage - Algorithms:C\:\\Users\\Cathy\\Zotero\\storage\\I4WQX7SD\\algorithms.html:text/html}
}

@inproceedings{gribbon_real-time_2003,
	title = {A {Real}-time {FPGA} {Implementation} of a {Barrel} {Distortion} {Correction} {Algorithm} with {Bilinear} {Interpolation}},
	url = {https://www.semanticscholar.org/paper/A-Real-time-FPGA-Implementation-of-a-Barrel-Distor-Gribbon/3d81ba49b2e2848c3392c980e1f0b8cab609f1b4},
	abstract = {This paper presents a novel FPGA implementation of a barrel distortion correction algorithm with a focus on reducing hardware complexity. In order to perform real-time correction in hardware the undistorted output pixels must be produced in raster order. To do this the implementation uses the current scan position in the undistorted image to calculate which pixel in the distorted image to display. The magnification factor needed in this calculation is stored in a special look-up table that reduces the complexity of the hardware design, without significant loss of precision. The application of bilinear interpolation to improve the quality of the corrected image is also discussed and a real-time approach is presented. This approach uses a novel method of row buffering to compensate for data bandwidth constraints when trying to obtain the required pixels for interpolation.},
	author = {Gribbon, K. T.},
	year = {2003},
	file = {Full Text PDF:C\:\\Users\\Cathy\\Zotero\\storage\\9RUZ6IQ7\\Gribbon - 2003 - A Real-time FPGA Implementation of a Barrel Distor.pdf:application/pdf}
}

@inproceedings{turk_face_1991,
	title = {Face recognition using eigenfaces},
	doi = {10.1109/CVPR.1991.139758},
	abstract = {An approach to the detection and identification of human faces is presented, and a working, near-real-time face recognition system which tracks a subject's head and then recognizes the person by comparing characteristics of the face to those of known individuals is described. This approach treats face recognition as a two-dimensional recognition problem, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. Face images are projected onto a feature space (`face space') that best encodes the variation among known face images. The face space is defined by the `eigenfaces', which are the eigenvectors of the set of faces; they do not necessarily correspond to isolated features such as eyes, ears, and noses. The framework provides the ability to learn to recognize new faces in an unsupervised manner},
	booktitle = {1991 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Proceedings}},
	author = {Turk, M. A. and Pentland, A. P.},
	month = jun,
	year = {1991},
	keywords = {unsupervised learning, Humans, Character recognition, Computational modeling, Computer vision, computerised pattern recognition, eigenfaces, eigenvalues and eigenfunctions, eigenvectors, Eyes, Face detection, face images, Face recognition, face recognition system, face space, feature space, Head, human faces, Image recognition, Nose, two-dimensional recognition},
	pages = {586--591},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Cathy\\Zotero\\storage\\BG2ZHHP2\\139758.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Cathy\\Zotero\\storage\\PGRWXH36\\Turk and Pentland - 1991 - Face recognition using eigenfaces.pdf:application/pdf}
}

@article{parkhi_deep_2015,
	title = {Deep {Face} {Recognition}},
	journal = {British Machine Vision Conference},
	author = {Parkhi, O. M. and Vedaldi, A and Zisserman, A},
	year = {2015}
}

@article{drap_exact_2016,
	title = {An {Exact} {Formula} for {Calculating} {Inverse} {Radial} {Lens} {Distortions}},
	volume = {16},
	issn = {1424-8220},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4934233/},
	doi = {10.3390/s16060807},
	abstract = {This article presents a new approach to calculating the inverse of radial distortions. The method presented here provides a model of reverse radial distortion, currently modeled by a polynomial expression, that proposes another polynomial expression where the new coefficients are a function of the original ones. After describing the state of the art, the proposed method is developed. It is based on a formal calculus involving a power series used to deduce a recursive formula for the new coefficients. We present several implementations of this method and describe the experiments conducted to assess the validity of the new approach. Such an approach, non-iterative, using another polynomial expression, able to be deduced from the first one, can actually be interesting in terms of performance, reuse of existing software, or bridging between different existing software tools that do not consider distortion from the same point of view.},
	number = {6},
	urldate = {2018-01-05},
	journal = {Sensors (Basel, Switzerland)},
	author = {Drap, Pierre and Lefèvre, Julien},
	month = jun,
	year = {2016},
	pmid = {27258288},
	pmcid = {PMC4934233},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Cathy\\Zotero\\storage\\HND28NSD\\Drap and Lefèvre - 2016 - An Exact Formula for Calculating Inverse Radial Le.pdf:application/pdf}
}

@article{upchurch_deep_2016,
	title = {Deep {Feature} {Interpolation} for {Image} {Content} {Changes}},
	url = {http://arxiv.org/abs/1611.05507},
	abstract = {We propose Deep Feature Interpolation (DFI), a new data-driven baseline for automatic high-resolution image transformation. As the name suggests, it relies only on simple linear interpolation of deep convolutional features from pre-trained convnets. We show that despite its simplicity, DFI can perform high-level semantic transformations like "make older/younger", "make bespectacled", "add smile", among others, surprisingly well - sometimes even matching or outperforming the state-of-the-art. This is particularly unexpected as DFI requires no specialized network architecture or even any deep network to be trained for these tasks. DFI therefore can be used as a new baseline to evaluate more complex algorithms and provides a practical answer to the question of which image transformation tasks are still challenging in the rise of deep learning.},
	urldate = {2018-01-05},
	journal = {arXiv:1611.05507 [cs]},
	author = {Upchurch, Paul and Gardner, Jacob and Pleiss, Geoff and Pless, Robert and Snavely, Noah and Bala, Kavita and Weinberger, Kilian},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.05507},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: First two authors contributed equally. Accepted by CVPR 2017. Code at https://github.com/paulu/deepfeatinterp},
	file = {arXiv\:1611.05507 PDF:C\:\\Users\\Cathy\\Zotero\\storage\\QJT9GMXE\\Upchurch et al. - 2016 - Deep Feature Interpolation for Image Content Chang.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Cathy\\Zotero\\storage\\LH56FA8L\\1611.html:text/html}
}

@incollection{ganesh_face_2012,
	title = {Face recognition by sparse representation},
	url = {https://people.eecs.berkeley.edu/~yang/paper/face_chapter.pdf},
	booktitle = {Compressed {Sensing}: {Theory} and {Applications}},
	publisher = {Cambridge University Press},
	author = {Ganesh, Arvind and Wagner, Andrew and Zhou, Zihan and Yang, Allen Y. and Ma, Yi and Wright, John},
	editor = {Eldar, Yonina C. and Kutyniok, GittaEditors},
	year = {2012},
	note = {DOI: 10.1017/CBO9780511794308.013},
	pages = {515--539}
}

@article{turk_eigenfaces_1991,
	title = {Eigenfaces for recognition},
	volume = {3},
	issn = {0898-929X},
	doi = {10.1162/jocn.1991.3.1.71},
	abstract = {We have developed a near-real-time computer system that can locate and track a subject's head, and then recognize the person by comparing characteristics of the face to those of known individuals. The computational approach taken in this system is motivated by both physiology and information theory, as well as by the practical requirements of near-real-time performance and accuracy. Our approach treats the face recognition problem as an intrinsically two-dimensional (2-D) recognition problem rather than requiring recovery of three-dimensional geometry, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. The system functions by projecting face images onto a feature space that spans the significant variations among known face images. The significant features are known as "eigenfaces," because they are the eigenvectors (principal components) of the set of faces; they do not necessarily correspond to features such as eyes, ears, and noses. The projection operation characterizes an individual face by a weighted sum of the eigenface features, and so to recognize a particular face it is necessary only to compare these weights to those of known individuals. Some particular advantages of our approach are that it provides for the ability to learn and later recognize new faces in an unsupervised manner, and that it is easy to implement using a neural network architecture.},
	language = {eng},
	number = {1},
	journal = {Journal of Cognitive Neuroscience},
	author = {Turk, M. and Pentland, A.},
	year = {1991},
	pmid = {23964806},
	pages = {71--86}
}

@article{hern_want_2016,
	chapter = {Technology},
	title = {Want to beat facial recognition? {Get} some funky tortoiseshell glasses},
	issn = {0261-3077},
	shorttitle = {Want to beat facial recognition?},
	url = {http://www.theguardian.com/technology/2016/nov/03/how-funky-tortoiseshell-glasses-can-beat-facial-recognition},
	abstract = {Eyewear printed with a wild pattern can be enough to fool commercial systems into misidentification, research shows},
	language = {en-GB},
	urldate = {2018-01-05},
	journal = {The Guardian},
	author = {Hern, Alex},
	month = nov,
	year = {2016},
	keywords = {3D printing, Facial recognition, Privacy, Technology, World news},
	file = {Snapshot:C\:\\Users\\Cathy\\Zotero\\storage\\LEJKL52M\\how-funky-tortoiseshell-glasses-can-beat-facial-recognition.html:text/html}
}

@incollection{huang_face_2011,
	title = {Face {Recognition} {Applications}},
	isbn = {978-0-85729-931-4 978-0-85729-932-1},
	url = {https://link.springer.com/chapter/10.1007/978-0-85729-932-1_24},
	abstract = {As one of the most nonintrusive biometrics, face recognition technology is becoming ever closer to people’s daily lives. Evidence of this is that in 2000 the International Civil Aviation Organization endorsed facial recognition as the most suitable biometrics for air travel. To our knowledge, no review papers are available on the newly enlarged application scenarios since then. We hope this chapter will be an extension of the previous studies. We review many face recognition applications that have already used face recognition technologies. This set of applications is a much larger super-set of previously reviewed. We also review some other new scenarios that will potentially utilize face recognition technologies in the near future.},
	language = {en},
	urldate = {2018-01-05},
	booktitle = {Handbook of {Face} {Recognition}},
	publisher = {Springer, London},
	author = {Huang, Thomas and Xiong, Ziyou and Zhang, Zhenqiu},
	year = {2011},
	note = {DOI: 10.1007/978-0-85729-932-1\_24},
	pages = {617--638},
	file = {Full Text PDF:C\:\\Users\\Cathy\\Zotero\\storage\\55NXJQ3W\\Huang et al. - 2011 - Face Recognition Applications.pdf:application/pdf;Snapshot:C\:\\Users\\Cathy\\Zotero\\storage\\U4X9TBRL\\978-0-85729-932-1_24.html:text/html}
}

@inproceedings{schroff_facenet:_2015,
	title = {{FaceNet}: {A} unified embedding for face recognition and clustering},
	shorttitle = {{FaceNet}},
	doi = {10.1109/CVPR.2015.7298682},
	abstract = {Despite significant recent advances in the field of face recognition [10, 14, 15, 17], implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure offace similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings asfeature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-artface recognition performance using only 128-bytes perface. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63\%. On YouTube Faces DB it achieves 95.12\%. Our system cuts the error rate in comparison to the best published result [15] by 30\% on both datasets.},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Schroff, F. and Kalenichenko, D. and Philbin, J.},
	month = jun,
	year = {2015},
	keywords = {convolution, neural nets, Face recognition, Accuracy, Artificial neural networks, data mining, deep convolutional network, embedding optimization, Face, face clustering, face patch matching, face recognition, FaceNet embedding, image matching, online triplet mining method, optimisation, pattern clustering, Principal component analysis, Standards, Training},
	pages = {815--823},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Cathy\\Zotero\\storage\\LCV7W8NR\\7298682.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Cathy\\Zotero\\storage\\9VD38DWY\\Schroff et al. - 2015 - FaceNet A unified embedding for face recognition .pdf:application/pdf}
}

@article{sun_meet_2017,
	title = {Meet the {Company} {That}’s {Using} {Face} {Recognition} to {Reshape} {China}’s {Tech} {Scene}},
	url = {https://www.technologyreview.com/s/608598/when-a-face-is-worth-a-billion-dollars/},
	abstract = {In China, you can use your face to get into your office, get on a train, or get a loan.},
	urldate = {2018-01-05},
	journal = {MIT Technology Review},
	author = {Sun, Yiting},
	month = aug,
	year = {2017},
	file = {Snapshot:C\:\\Users\\Cathy\\Zotero\\storage\\SDMNEGLU\\when-a-face-is-worth-a-billion-dollars.html:text/html}
}

@misc{savarese_camera_2015,
	title = {Camera {Calibration} {Lecture} {Notes}},
	url = {http://cvgl.stanford.edu/teaching/cs231a_winter1415/lecture/lecture3_camera_calibration_notes.pdf},
	author = {Savarese, Silvio},
	month = jan,
	year = {2015}
}

@misc{vidal_eigenfaces_2008,
	title = {Eigenfaces for {Face} {Detection}/{Recognition} ({Course} {Notes})},
	url = {http://www.vision.jhu.edu/teaching/vision08/},
	journal = {Computer Vision -- Spring 2008},
	author = {Vidal, Rene},
	year = {2008}
}
